# FreshKart - Pipeline de consolidation des ventes

Pipeline ETL quotidien pour traiter et agrÃ©ger les donnÃ©es de ventes par ville et canal.

## ğŸ“‹ PrÃ©requis

- Python 3.12+
- UV (recommandÃ©) ou pip/venv
- SQLite3
- AccÃ¨s cron (Linux/macOS) ou Task Scheduler (Windows)

## ğŸš€ Installation

### Option 1 : Avec UV (recommandÃ©)

```bash
# Cloner le projet
git clone https://github.com/GMartin-Data/de1-freshkart.git
cd freshkart-pipeline

# Initialiser avec UV
uv sync

# CrÃ©er les dossiers nÃ©cessaires
mkdir -p data/input data/output data/out logs
```

### Option 2 : Avec pip/venv

```bash
# Cloner le projet
git clone <repo-url>
cd freshkart-pipeline

# CrÃ©er et activer l'environnement virtuel
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
# ou .venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install pandas

# CrÃ©er les dossiers nÃ©cessaires
mkdir -p data/input data/out logs
```

## ğŸ“ Structure des donnÃ©es

Placez vos fichiers sources dans `data/input/` :

```
data/input/
â”œâ”€â”€ customers.csv              # RÃ©fÃ©rentiel clients (ville, statut actif)
â”œâ”€â”€ refunds.csv                # Historique des remboursements
â””â”€â”€ orders_YYYY-MM-DD.json     # Commandes quotidiennes avec articles
```

**Format attendu** :

- `customers.csv` : customer_id, first_name, last_name, email, city, is_active
- `refunds.csv` : refund_id, order_id, amount, reason, created_at
- `orders_*.json` : order_id, customer_id, channel, created_at, payment_status, items[]

## ğŸ’» Utilisation

### ExÃ©cution manuelle

```bash
# Avec UV
uv run pipeline.py <YYYY-MM-DD>

# Avec venv
source .venv/bin/activate
python pipeline.py <YYYY-MM-DD>
```

### Automatisation quotidienne (Linux/macOS)

```bash
# Rendre le script exÃ©cutable
chmod +x run_daily.sh

# Tester manuellement
./run_daily.sh

# Configurer le cron (6h du matin, timezone Paris)
crontab -e

# Ajouter cette ligne :
TZ=Europe/Paris
0 6 * * * cd $HOME/freshkart-pipeline && ./run_daily.sh >> logs/pipeline.log 2>&1
```

**Le script `run_daily.sh` :**

- Calcule automatiquement la date de la veille (J-1)
- VÃ©rifie l'existence des fichiers requis
- DÃ©tecte automatiquement l'environnement (UV ou venv)
- Log toutes les exÃ©cutions dans `logs/pipeline.log`

## ğŸ“Š RÃ¨gles mÃ©tier appliquÃ©es

1. **Clients** : Uniquement les clients actifs (`is_active = true`)
2. **Commandes** : Uniquement les commandes payÃ©es (`payment_status = 'paid'`)
3. **Articles** : Rejet des prix unitaires nÃ©gatifs (loggÃ©s dans `rejected_items_*.csv`)
4. **DÃ©duplication** : PremiÃ¨re occurrence conservÃ©e en cas de doublon sur `order_id`
5. **Remboursements** : Montants nÃ©gatifs agrÃ©gÃ©s par commande

## ğŸ“¤ Outputs gÃ©nÃ©rÃ©s

```
data/out/
â”œâ”€â”€ daily_summary_YYYYMMDD.csv       # AgrÃ©gats par ville/canal (CSV principal)
â”œâ”€â”€ rejected_items_YYYYMMDD.csv      # Items rejetÃ©s (si prix nÃ©gatifs)
â””â”€â”€ sales.db                         # Base SQLite cumulative
    â”œâ”€â”€ orders_clean                 # DÃ©tails par commande
    â””â”€â”€ daily_city_sales             # AgrÃ©gats quotidiens

logs/
â””â”€â”€ pipeline.log                     # Historique d'exÃ©cution
```

### Format du CSV quotidien

Colonnes : `date;city;channel;orders_count;unique_customers;items_sold;gross_revenue_eur;refunds_eur;net_revenue_eur`

Exemple :

```csv
date;city;channel;orders_count;unique_customers;items_sold;gross_revenue_eur;refunds_eur;net_revenue_eur
2025-03-01;Paris;web;7;7;53;776.8;-6.76;770.04
2025-03-01;Lyon;app;7;7;62;730.3;-10.86;719.44
```

## ğŸ” VÃ©rification

```bash
# VÃ©rifier les fichiers gÃ©nÃ©rÃ©s
ls -lh data/out/

# Inspecter la base SQLite
sqlite3 data/out/sales.db "SELECT COUNT(*) FROM orders_clean;"
sqlite3 data/out/sales.db "SELECT * FROM daily_city_sales ORDER BY date DESC LIMIT 5;"

# Consulter les logs
tail -f logs/pipeline.log
```

## ğŸ› ï¸ DÃ©veloppement

### Architecture du pipeline

```
1. CHARGEMENT    â†’ Lecture des fichiers sources
2. NETTOYAGE     â†’ Application des rÃ¨gles mÃ©tier + filtres
3. ENRICHISSEMENT â†’ Jointure avec ville client
4. CALCULS       â†’ Revenus bruts/nets par commande
5. AGRÃ‰GATION    â†’ MÃ©triques par ville/canal/date
6. EXPORT        â†’ CSV quotidien + SQLite cumulatif
```

### Exploration initiale

Voir le notebook Jupyter `exploration.ipynb` pour le processus de dÃ©veloppement et les analyses exploratoires.

### Linter et formatage

```bash
# Formater le code
ruff format pipeline.py

# VÃ©rifier le code
ruff check pipeline.py
```

## âš ï¸ Gestion des erreurs

Le pipeline s'arrÃªte et log les erreurs dans les cas suivants :

- Fichier `orders_YYYY-MM-DD.json` introuvable pour la date demandÃ©e
- Fichiers `customers.csv` ou `refunds.csv` manquants
- Format de date invalide (attendu : YYYY-MM-DD)
- Erreur d'exÃ©cution Python

Consultez `logs/pipeline.log` pour diagnostiquer les problÃ¨mes.

## ğŸ“ Notes techniques

- **DÃ©normalisation** : Les items incluent directement les mÃ©tadonnÃ©es de commande pour optimiser les jointures
- **Refunds** : FiltrÃ©s sur les order_id valides avant agrÃ©gation (optimisation performance)
- **SQLite** : Mode append pour accumulation historique (attention aux doublons si re-exÃ©cution)
- **Net revenue nÃ©gatif** : Possible si remboursements > commande (gestes commerciaux exceptionnels)
